{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Experiments and reference implementations.\n",
    "output-file: index.html\n",
    "title: Sinusoidal Frequency Estimation by Gradient Descent\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from sinusoidal_gradient_descent.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size: large\"><a href=\"https://arxiv.org/abs/2210.14476\">arXiv paper üìù</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is becoming increasingly popular to incorporate signal generators and processors into neural network architecures.\n",
    "When implemented via differentiable programming, networks can be trained to control signal processors with defined gradients using standard first order optimizers. \n",
    "This technique is commonly known as [Differentiable Digital Signal Processing (DDSP)](https://magenta.tensorflow.org/ddsp).\n",
    "\n",
    "But there's a problem... a pretty fundamental operation in many audio signal processing tasks ‚Äî namely, matching the frequency of a sinusoid ‚Äî can not be easily solved this way.\n",
    "\n",
    "Just look at what happens when we try to use gradient descent to find the frequency of a single differentiable sinusoid..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import math\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting frequency: 1.002\n",
      "--- Step: 1000 ---\n",
      "Predicted frequency: 0.969\n",
      "Target frequency: 0.250\n",
      "--- Step: 2000 ---\n",
      "Predicted frequency: 0.969\n",
      "Target frequency: 0.250\n",
      "--- Step: 3000 ---\n",
      "Predicted frequency: 0.969\n",
      "Target frequency: 0.250\n",
      "--- Step: 4000 ---\n",
      "Predicted frequency: 0.969\n",
      "Target frequency: 0.250\n",
      "--- Step: 5000 ---\n",
      "Predicted frequency: 0.969\n",
      "Target frequency: 0.250\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "n = torch.arange(N)\n",
    "\n",
    "torch.random.manual_seed(1000)\n",
    "predicted_freq = (torch.rand(1) * math.pi).requires_grad_()\n",
    "print(f\"Starting frequency: {predicted_freq.item():.3f}\")\n",
    "\n",
    "target_freq = torch.tensor(0.25)\n",
    "target_signal = torch.cos(target_freq * n)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.SGD([predicted_freq], lr=3e-4)\n",
    "\n",
    "for step in range(5000):\n",
    "    predicted_signal = torch.cos(predicted_freq * n)\n",
    "    loss = criterion(predicted_signal, target_signal)\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if (step + 1) % 1000 == 0:\n",
    "        print(f\"--- Step: {step + 1} ---\")\n",
    "        print(f\"Predicted frequency: {predicted_freq.item():.3f}\")\n",
    "        print(f\"Target frequency: {target_freq.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start at almost any value of `predicted_freq` and we'll still end up failing to find the correct frequency.\n",
    "[Our paper](https://arxiv.org/abs/2210.14476) goes into more detail about why this happens, but suffice it to say that this prevents DDSP techniques from being applied to many tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work aims to resolve this issue. We do so by replacing the differentiable sinusoid with a surrogate: the real part of an exponentiated complex number. This gives us an exponentially decaying sinusoid, and the Wirtinger derivatives of this operation in the complex plane lead our optimizer to the correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinusoidal_gradient_descent.core import complex_oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting frequency: 1.002\n",
      "--- Step: 1000 ---\n",
      "Predicted frequency: 0.952\n",
      "Target frequency: 0.250\n",
      "--- Step: 2000 ---\n",
      "Predicted frequency: 0.549\n",
      "Target frequency: 0.250\n",
      "--- Step: 3000 ---\n",
      "Predicted frequency: 0.241\n",
      "Target frequency: 0.250\n",
      "--- Step: 4000 ---\n",
      "Predicted frequency: 0.250\n",
      "Target frequency: 0.250\n",
      "--- Step: 5000 ---\n",
      "Predicted frequency: 0.250\n",
      "Target frequency: 0.250\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "n = torch.arange(N)\n",
    "\n",
    "torch.random.manual_seed(1000)\n",
    "starting_freq = torch.rand(1) * math.pi\n",
    "predicted_z = torch.exp(1j * starting_freq)\n",
    "predicted_z.detach_().requires_grad_(True)\n",
    "print(f\"Starting frequency: {predicted_z.angle().abs().item():.3f}\")\n",
    "\n",
    "target_freq = torch.tensor(0.25)\n",
    "target_signal = torch.cos(target_freq * n)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.SGD([predicted_z], lr=3e-4)\n",
    "\n",
    "for step in range(5000):\n",
    "    predicted_signal = complex_oscillator(predicted_z, N=N, reduce=True)\n",
    "    loss = criterion(predicted_signal, target_signal)\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    predicted_z.grad = predicted_z.grad / predicted_z.grad.abs()\n",
    "    optimiser.step()\n",
    "\n",
    "    if (step + 1) % 1000 == 0:\n",
    "        print(f\"--- Step: {step + 1} ---\")\n",
    "        print(f\"Predicted frequency: {predicted_z.angle().abs().item():.3f}\")\n",
    "        print(f\"Target frequency: {target_freq.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complex surrogate described in the paper can be easily implemented in PyTorch in a number of ways. We present three of these below as reference implementations. In all cases, for a complex parameter, PyTorch computes the Wirtinger derivative by default, meaning that no custom backward pass is necessary. [Click here for more info](https://pytorch.org/docs/stable/notes/autograd.html#autograd-for-complex-numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64\n",
    "freq = torch.tensor(0.7)\n",
    "cosine_reference = torch.cos(freq * torch.arange(N))\n",
    "\n",
    "z = torch.exp(1j * freq)  # complex parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 1: direct exponentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation produces the signal exactly as described in the paper: by taking the exponential of $z$.\n",
    "The major downside here is that numerical instability can become an issue for values of $z>1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_oscillator_direct(z: torch.complex, N: int):\n",
    "    \"\"\"Implements the complex surrogate by direct exponentiation.\"\"\"\n",
    "    n = torch.arange(N)\n",
    "    return (z ** n).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_osc_output = complex_oscillator_direct(z, N)\n",
    "\n",
    "torch.testing.assert_close(direct_osc_output, cosine_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 2: cumulative product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation takes the cumulative product across a time series of complex parameters $z_{1:N}$. As with direct exponentiation, instability can become an issue, as can loss of precision. The advantage of this approach, however, is that it can be extended to a time-varying parameter.\n",
    "\n",
    "Note that here we prepend the sequence with an initial $1$ to correspond to $z^0$, and thus truncate the final entry of $z$, retaining only $z_{1:N-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_oscillator_cumprod(z: torch.complex):\n",
    "    \"\"\"Implements the complex surrogate by taking the cumulative product along the time\n",
    "    dimension.\"\"\"\n",
    "    initial = torch.ones(*z.shape[:-1], 1, dtype=z.dtype, device=z.device)\n",
    "    z_cat = torch.cat([initial, z], dim=-1)[:-1]\n",
    "\n",
    "    return torch.cumprod(z_cat, dim=-1).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumprod_osc_output = complex_oscillator_cumprod(z.repeat(N))\n",
    "\n",
    "torch.testing.assert_close(cumprod_osc_output, cosine_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 3: directly damped sinusoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation addresses the issues with numerical instability seen with\n",
    "direct exponentiation and cumulative multiplication by instead explicitly\n",
    "computing the parameters of the damped sinusoidal equivalent of the surrogate.\n",
    "This could further be adapted to time-varying $z$ by utilising a cumulative\n",
    "summation to compute the sinusoid's phase and a cumulative product to compute\n",
    "instantaneous amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_oscillator_damped(z: torch.complex, N: int):\n",
    "    \"\"\"Implements the complex surrogate by explicitly computing the parameters of the\n",
    "    damped sinusoid equivalent.\"\"\"\n",
    "    n = torch.arange(N)\n",
    "    return z.abs() * torch.cos(z.angle() * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "damped_osc_output = complex_oscillator_damped(z, N)\n",
    "\n",
    "torch.testing.assert_close(damped_osc_output, cosine_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the code and reproduce our experiments, we recommend installing the package inside a virtual environment of your choice:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/ben-hayes/sinusoidal-gradient-descent.git\n",
    "cd sinusoidal gradient-descent\n",
    "conda create --name sin-gd\n",
    "conda activate sin-gd\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "You will also need to ensure you have a version of PyTorch appropriate to your\n",
    "platform installed. See [here for more\n",
    "information](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "The experiments can then be run using the `evaluate_estimator` script:\n",
    "\n",
    "```bash\n",
    "evaluate_estimator -cn multi_fft_2\n",
    "```\n",
    "\n",
    "Substitute `multi_fft_2` for any of the configuration names from the `estimator_config` directory to run different experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On nbdev\n",
    "\n",
    "This project's code was written using [nbdev](https://nbdev.fast.ai/), which allows Python modules to be created with accompanying documentation in a fast notebook-style workflow.\n",
    "Our code can therefore either be explored in module form (in the `sinusoidal_gradient_descent`) folder, in notebook form (in the `nbs` folder), or [as a website](https://benhayes.net/sinusoidal-gradient-descent)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sin-gd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7369e33a2a76aa423a25ee4fa312e7324fdb42db15274e2163df956c91b9ed2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
