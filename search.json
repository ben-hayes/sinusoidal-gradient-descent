[
  {
    "objectID": "evaluate_estimator.html",
    "href": "evaluate_estimator.html",
    "title": "Estimator Evaluation",
    "section": "",
    "text": "source\n\nSinusoidEvaluationDataset\n\n SinusoidEvaluationDataset (signal_length:int=4096,\n                            freq_range:Tuple[float]=(0.0, 0.5),\n                            amp_range:Tuple[float]=(0.0, 1.0),\n                            phase_range:Tuple[float]=(0.0,\n                            6.283185307179586),\n                            snr_range:Tuple[float]=(0.0, 30.0),\n                            n_freqs:int=100, n_amps:int=100,\n                            n_phases:int=100, n_snrs:int=7,\n                            evaluate_phase:bool=False,\n                            initial_phase:float=0.0)\n\nImplements a synthetic dataset of sinusoids in white Gaussian noise for the single sinusoid evaluation task.\n\nsource\n\n\nMultiSinusoidEvaluationDataset\n\n MultiSinusoidEvaluationDataset (signal_length:int=4096,\n                                 n_components:int=4,\n                                 freq_range:Tuple[float]=(0.0, 0.5),\n                                 amp_range:Tuple[float]=(0.0, 1.0),\n                                 snr_range:Tuple[float]=(0.0, 30.0),\n                                 n_samples:int=100, n_snrs:int=7,\n                                 initial_phase:float=0.0,\n                                 dataset_seed:int=0,\n                                 enable_noise:bool=True)\n\nImplements a synthetic dataset of sinusoidal mixtures for the multi-sinusoid estimation task.\n\nsource\n\n\nsample_initial_predictions\n\n sample_initial_predictions (n_sinusoids:int, freq_range:Tuple[float],\n                             amp_range:Tuple[float], initial_phase:float,\n                             invert_sigmoid:bool=False,\n                             batch_size:Optional[int]=None,\n                             all_random_in_batch:bool=False, seed:int=0,\n                             device:str='cpu', flatten:bool=False)\n\nSamples initial parameters for sinusoidal frequency estimation\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_sinusoids\nint\n\nThe number of sinusoidal components\n\n\nfreq_range\ntyping.Tuple[float]\n\nThe range of possible frequencies\n\n\namp_range\ntyping.Tuple[float]\n\nThe range of possible amplitudes\n\n\ninitial_phase\nfloat\n\nThe initial phase of the sinusoids\n\n\ninvert_sigmoid\nbool\nFalse\nWhether to invert the sigmoid function when sampling the amplitudes\n\n\nbatch_size\ntyping.Optional[int]\nNone\nThe batch size of initial predictions\n\n\nall_random_in_batch\nbool\nFalse\nIf true, all predictions in a batch will be sampled randomly. If false, one randomly sampled prediction will be repeated across the batch dimension.\n\n\nseed\nint\n0\nThe random seed\n\n\ndevice\nstr\ncpu\nThe device to place the initial predictions on\n\n\nflatten\nbool\nFalse\nWhether to flatten the initial predictions\n\n\n\n\nsource\n\n\nevaluation_loop\n\n evaluation_loop (dataloader:torch.utils.data.dataloader.DataLoader,\n                  loss_cfg:omegaconf.dictconfig.DictConfig,\n                  optimizer_cfg:omegaconf.dictconfig.DictConfig,\n                  amplitude_estimator_cfg:omegaconf.dictconfig.DictConfig,\n                  metric_fn_cfg:omegaconf.dictconfig.DictConfig,\n                  initial_params:Tuple[torch.Tensor],\n                  use_real_sinusoid_baseline:bool=False,\n                  use_global_amp:bool=True,\n                  saturate_global_amp:bool=False,\n                  normalise_complex_grads:bool=False, mode:str='multi',\n                  device:Union[torch.device,str]='cpu', n_steps:int=1000,\n                  log_interval:int=100, seed:int=0)\n\nRuns the experimental evaluation\n\nsource\n\n\nrun\n\n run (cfg:omegaconf.dictconfig.DictConfig)\n\nRuns the estimator evaluation"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\ncomplex_oscillator\n\n complex_oscillator (z:torch.ComplexType,\n                     initial_phase:Optional[torch.ComplexType]=None,\n                     N:int=2048, constrain:bool=False, reduce:bool=False)\n\nGenerates an exponentially decaying sinusoid from a complex number.\nUsing the above oscillator to generate a signal at the Nyquist frequency:\n\nfreq = 0.5\namp = 0.8\ninitial_phase = 1.0 * torch.exp(torch.tensor([1j * math.pi / 2]))\nN = 32\nz = torch.polar(torch.tensor([amp]), torch.tensor([freq * 2 * math.pi]))\nsig = complex_oscillator(z, initial_phase, N, reduce=True)\n\n\nplt.plot(sig)\n\n\n\n\n\nsource\n\n\nestimate_amplitude\n\n estimate_amplitude (z, N, constrain=False, representation='fft')\n\n\nz = 0.99 * torch.exp(1j * torch.rand(2, 6))\nN = 2048\n\nestimate_amplitude(z, N, representation=\"lin\")\n\ntensor([[0.0525, 0.0681, 0.0640, 0.0530, 0.0635, 0.0679],\n        [0.0522, 0.0522, 0.0522, 0.0517, 0.0528, 0.0648]])\n\n\n\nsource\n\n\nfft_loss\n\n fft_loss (pred_signal, target_signal, lin_l1:float=1.0, lin_l2:float=0.0,\n           lin_huber:float=0.0, log_l1:float=0.0, log_l2:float=0.0,\n           log_huber:float=0.0, reduce_freq:str='mean',\n           reduce_batch:str='sum', eps:float=1e-08)\n\n\nsource\n\n\nget_reduce_fn\n\n get_reduce_fn (reduce:str)\n\n\nsource\n\n\nreal_oscillator\n\n real_oscillator (freqs, amps, phases=None, N=2048)\n\nGenerate real sinusoids with given frequencies, amplitudes, and phases."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sinusoidal Frequency Estimation by Gradient Descent",
    "section": "",
    "text": "arXiv paper 📝\nIt is becoming increasingly popular to incorporate signal generators and processors into neural network architecures. When implemented via differentiable programming, networks can be trained to control signal processors with defined gradients using standard first order optimizers. This technique is commonly known as Differentiable Digital Signal Processing (DDSP).\nBut there’s a problem… a pretty fundamental operation in many audio signal processing tasks — namely, matching the frequency of a sinusoid — can not be easily solved this way.\nJust look at what happens when we try to use gradient descent to find the frequency of a single differentiable sinusoid…\nWe can start at almost any value of predicted_freq and we’ll still end up failing to find the correct frequency. Our paper goes into more detail about why this happens, but suffice it to say that this prevents DDSP techniques from being applied to many tasks.\nThis work aims to resolve this issue. We do so by replacing the differentiable sinusoid with a surrogate: the real part of an exponentiated complex number. This gives us an exponentially decaying sinusoid, and the Wirtinger derivatives of this operation in the complex plane lead our optimizer to the correct solution.\nFor example:"
  },
  {
    "objectID": "index.html#implementations",
    "href": "index.html#implementations",
    "title": "Sinusoidal Frequency Estimation by Gradient Descent",
    "section": "Implementations",
    "text": "Implementations\nThe complex surrogate described in the paper can be easily implemented in PyTorch in a number of ways. We present three of these below as reference implementations. In all cases, for a complex parameter, PyTorch computes the Wirtinger derivative by default, meaning that no custom backward pass is necessary. Click here for more info.\n\nN = 64\nfreq = torch.tensor(0.7)\ncosine_reference = torch.cos(freq * torch.arange(N))\n\nz = torch.exp(1j * freq)  # complex parameter\n\n\nImplementation 1: direct exponentiation\nThis implementation produces the signal exactly as described in the paper: by taking the exponential of \\(z\\). The major downside here is that numerical instability can become an issue for values of \\(z>1\\).\n\ndef complex_oscillator_direct(z: torch.complex, N: int):\n    \"\"\"Implements the complex surrogate by direct exponentiation.\"\"\"\n    n = torch.arange(N)\n    return (z ** n).real\n\n\ndirect_osc_output = complex_oscillator_direct(z, N)\n\ntorch.testing.assert_close(direct_osc_output, cosine_reference)\n\nTiming the operation on an Intel i5 2GHz Quad Core CPU:\n\n\n\n15 µs ± 740 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\nImplementation 2: cumulative product\nThis implementation takes the cumulative product across a time series of complex parameters \\(z_{1:N}\\). As with direct exponentiation, instability can become an issue, as can loss of precision. The advantage of this approach, however, is that it can be extended to a time-varying parameter.\nNote that here we prepend the sequence with an initial \\(1\\) to correspond to \\(z^0\\), and thus truncate the final entry of \\(z\\), retaining only \\(z_{1:N-1}\\).\n\ndef complex_oscillator_cumprod(z: torch.complex):\n    \"\"\"Implements the complex surrogate by taking the cumulative product along the time\n    dimension.\"\"\"\n    initial = torch.ones(*z.shape[:-1], 1, dtype=z.dtype, device=z.device)\n    z_cat = torch.cat([initial, z], dim=-1)[:-1]\n\n    return torch.cumprod(z_cat, dim=-1).real\n\n\ncumprod_osc_output = complex_oscillator_cumprod(z.repeat(N))\n\ntorch.testing.assert_close(cumprod_osc_output, cosine_reference)\n\nTiming the operation on an Intel i5 2GHz Quad Core CPU:\n\nz_repeat = z.repeat(N)\n\n18.3 µs ± 904 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\nImplementation 3: directly damped sinusoid\nThis implementation limits the issues with numerical stability to only the amplitude parameter by instead explicitly computing the parameters of the damped sinusoidal equivalent of the surrogate. This could further be adapted to time-varying \\(z\\) by utilising a cumulative summation to compute the sinusoid’s phase and a cumulative product to compute instantaneous amplitude. Adopting an angular cumsum operation to accumulate sinusoidal phase would further improve numerical precision.\n\ndef complex_oscillator_damped(z: torch.complex, N: int):\n    \"\"\"Implements the complex surrogate by explicitly computing the parameters of the\n    damped sinusoid equivalent.\"\"\"\n    n = torch.arange(N)\n    return (z.abs() ** n) * torch.cos(z.angle() * n)\n\n\ndamped_osc_output = complex_oscillator_damped(z, N)\n\ntorch.testing.assert_close(damped_osc_output, cosine_reference)\n\nTiming the operation on an Intel i5 2GHz Quad Core CPU:\n\n\n\n27.2 µs ± 346 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "index.html#running-this-code",
    "href": "index.html#running-this-code",
    "title": "Sinusoidal Frequency Estimation by Gradient Descent",
    "section": "Running this code",
    "text": "Running this code\nTo run the code and reproduce our experiments, we recommend installing the package inside a virtual environment of your choice:\ngit clone https://github.com/ben-hayes/sinusoidal-gradient-descent.git\ncd sinusoidal-gradient-descent\nconda create --name sin-gd\nconda activate sin-gd\npip install -e .\nYou will also need to ensure you have a version of PyTorch appropriate to your platform installed. See here for more information.\nThe experiments can then be run using the evaluate_estimator script:\nevaluate_estimator -cn multi_fft_2\nSubstitute multi_fft_2 for any of the configuration names from the estimator_config directory to run different experiments.\n\nOn nbdev\nThis project’s code was written using nbdev, which allows Python modules to be created with accompanying documentation in a fast notebook-style workflow. Our code can therefore either be explored in module form (in the sinusoidal_gradient_descent) folder, in notebook form (in the nbs folder), or as a website."
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "source\n\nitakura_saito\n\n itakura_saito (x, y)\n\n\nsource\n\n\nspectral_mse\n\n spectral_mse (x, y)\n\n\nsource\n\n\nspectral_mse_db\n\n spectral_mse_db (x, y)\n\n\nsource\n\n\nlog_spectral_distance\n\n log_spectral_distance (x, y)\n\n\nsource\n\n\npower_spectrum\n\n power_spectrum (x)\n\n\nsource\n\n\nmagnitude_spectrum\n\n magnitude_spectrum (x)\n\n\nsource\n\n\nsingle_sinusoid_metrics\n\n single_sinusoid_metrics (target_signal:torch.Tensor,\n                          target_freq:torch.Tensor,\n                          target_amp:torch.Tensor,\n                          target_snr:torch.Tensor,\n                          predicted_signal:torch.Tensor,\n                          predicted_freq:torch.Tensor,\n                          predicted_amp:torch.Tensor)\n\nCalculate metrics for single sinusoid evaluation.\n\nsource\n\n\nmin_lap_cost\n\n min_lap_cost (target, predicted, unsqueeze=False)\n\nCalculate the minimum lap cost between two sets of points.\n\nsource\n\n\nchamfer_distance\n\n chamfer_distance (target, predicted, unsqueeze=False)\n\nCalculate the Chamfer distance between two sets of points.\n\na = torch.randn(16, 3)\nb = torch.randn(16, 3)\n\nchamfer_distance(a, b, unsqueeze=True).shape\n\ntorch.Size([16])\n\n\n\nsource\n\n\nmulti_sinusoid_metrics\n\n multi_sinusoid_metrics (target_signal:torch.Tensor,\n                         target_freq:torch.Tensor,\n                         target_amp:torch.Tensor, target_snr:torch.Tensor,\n                         predicted_signal:torch.Tensor,\n                         predicted_freq:torch.Tensor,\n                         predicted_amp:torch.Tensor)\n\nCalculate metrics for single sinusoid evaluation."
  }
]